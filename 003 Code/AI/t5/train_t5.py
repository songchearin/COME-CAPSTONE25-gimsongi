import json
from datasets import Dataset
from transformers import (
    T5ForConditionalGeneration,
    T5TokenizerFast as T5Tokenizer,
    TrainingArguments,
    Trainer
)
import numpy as np
import os

# --- 1. ê¸°ë³¸ ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì¤€ë¹„ ---

MODEL_NAME = "paust/pko-t5-base"
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

DATASET_PATH = "./train_dataset.json"
with open(DATASET_PATH, 'r', encoding='utf-8') as f:
    raw_data = json.load(f)

# ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬
np.random.shuffle(raw_data)
split_point = int(len(raw_data) * 0.9)
train_data = raw_data[:split_point]
eval_data = raw_data[split_point:]

print(f"í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(eval_data)}ê°œ")

train_dataset = Dataset.from_list(train_data)
eval_dataset = Dataset.from_list(eval_data)

# --- 2. ë°ì´í„° ì „ì²˜ë¦¬ ---

def preprocess_function(examples):
    inputs = tokenizer(examples["input"], padding="max_length", truncation=True, max_length=128)
    labels = tokenizer(examples["output"], padding="max_length", truncation=True, max_length=128)
    inputs["labels"] = labels["input_ids"]
    return inputs

tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)

# --- 3. ëª¨ë¸ í•™ìŠµ ì„¤ì • (ìˆ˜ë™ ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ ë‹¨ìˆœí™”) ---

# ğŸ”¥ í•œ ë²ˆì— 1 epochë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì‹¤ì œ í•™ìŠµì€ ì•„ë˜ì˜ for loopê°€ ì œì–´í•©ë‹ˆë‹¤.
MAX_EPOCHS = 10 # ìµœëŒ€ í•™ìŠµí•  ì—í¬í¬ ìˆ˜

training_args = TrainingArguments(
    output_dir="./t5_finetune_results",
    num_train_epochs=1, # í•­ìƒ 1ë¡œ ê³ ì •
    per_device_train_batch_size=4,
    logging_steps=50,
    logging_dir='./logs',
    # ğŸ”¥ ë²„ì „ ì¶©ëŒì„ ì¼ìœ¼í‚¤ëŠ” ì¡°ê¸° ì¢…ë£Œ ê´€ë ¨ ì¸ìë“¤ì„ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.
)

# --- 4. Trainer ì„¤ì • ---

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    # ğŸ”¥ ì½œë°± ê¸°ëŠ¥ë„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
)

# --- 5. ìˆ˜ë™ ì¡°ê¸° ì¢…ë£Œë¥¼ í¬í•¨í•œ í•™ìŠµ ë£¨í”„ ì‹œì‘ ---

print("â–¶ ìˆ˜ë™ ì¡°ê¸° ì¢…ë£Œ ê¸°ëŠ¥ìœ¼ë¡œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

best_eval_loss = float("inf")
patience_counter = 0
early_stopping_patience = 3 # 3ë²ˆ ì—°ì† ì„±ëŠ¥ í–¥ìƒì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
SAVE_PATH = "./my_finetuned_t5_model"

for epoch in range(MAX_EPOCHS):
    print(f"--- Epoch {epoch + 1}/{MAX_EPOCHS} ---")
    
    # 1 epoch í•™ìŠµ
    trainer.train()
    
    # ëª¨ë¸ í‰ê°€
    eval_results = trainer.evaluate()
    current_eval_loss = eval_results["eval_loss"]
    print(f"Epoch {epoch + 1} - ê²€ì¦ ì†ì‹¤(Validation Loss): {current_eval_loss}")
    
    # ìµœê³  ì„±ëŠ¥ì¸ì§€ í™•ì¸
    if current_eval_loss < best_eval_loss:
        best_eval_loss = current_eval_loss
        patience_counter = 0
        print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°œê²¬! (ì†ì‹¤: {best_eval_loss:.4f})")
        print(f"   ëª¨ë¸ì„ '{SAVE_PATH}'ì— ì €ì¥í•©ë‹ˆë‹¤...")
        trainer.save_model(SAVE_PATH)
        tokenizer.save_pretrained(SAVE_PATH)
    else:
        patience_counter += 1
        print(f"   ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ. (Patience: {patience_counter}/{early_stopping_patience})")
    
    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
    if patience_counter >= early_stopping_patience:
        print(f"ğŸ”´ {early_stopping_patience}ë²ˆ ì—°ì†ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šì•„ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

print("âœ… ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"âœ… ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì´ '{SAVE_PATH}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
