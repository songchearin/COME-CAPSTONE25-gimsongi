# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
import numpy as np
import os
import datetime
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard
from sklearn.model_selection import StratifiedShuffleSplit
import matplotlib.pyplot as plt # ğŸŒŸ ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# --- âš™ï¸ ì„¤ì •: ì´ ìŠ¤ìœ„ì¹˜ë¡œ ëª¨ë“œë¥¼ ë³€ê²½í•˜ì„¸ìš” ---
# True: model_save_pathì— ìˆëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì´ì–´ì„œ í•™ìŠµí•˜ê±°ë‚˜, êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ì—¬ ì „ì´ í•™ìŠµí•©ë‹ˆë‹¤.
# False: ê¸°ì¡´ì²˜ëŸ¼ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.
CONTINUE_TRAINING = False
# -----------------------------------------

# ğŸŒŸ [ì¶”ê°€] í•™ìŠµ ê³¼ì •ì„ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ì£¼ëŠ” í•¨ìˆ˜
def plot_training_history(history, model_save_path):
    """í›ˆë ¨ ê²°ê³¼ë¥¼ ë°›ì•„ ì •í™•ë„ì™€ ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # ì •í™•ë„ ê·¸ë˜í”„
    ax1.plot(history.history['accuracy'], label='Train Accuracy')
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax1.set_title('Model Accuracy')
    ax1.set_ylabel('Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.legend(loc='upper left')
    ax1.grid(True)

    # ì†ì‹¤ ê·¸ë˜í”„
    ax2.plot(history.history['loss'], label='Train Loss')
    ax2.plot(history.history['val_loss'], label='Validation Loss')
    ax2.set_title('Model Loss')
    ax2.set_ylabel('Loss')
    ax2.set_xlabel('Epoch')
    ax2.legend(loc='upper left')
    ax2.grid(True)
    
    # ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
    save_path = os.path.splitext(model_save_path)[0] + '_history.png'
    plt.savefig(save_path)
    print(f"ğŸ“Š í•™ìŠµ ê³¼ì • ê·¸ë˜í”„ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # plt.show() # ì£¼í”¼í„° ë…¸íŠ¸ë¶ ë“±ì—ì„œ ë°”ë¡œ ë³´ë ¤ë©´ ì´ ì¤„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.

def train_lstm_model_dual(X_path, y_path, model_save_path):
    """
    ì „ì²˜ë¦¬ëœ ì–‘ì† ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì„±ëŠ¥ì„ ë¶„ì„ ë° ì‹œê°í™”í•©ë‹ˆë‹¤.
    - CONTINUE_TRAINING=Trueì¼ ë•Œ, í´ë˜ìŠ¤ ê°œìˆ˜ê°€ ê°™ìœ¼ë©´ ì´ì–´ì„œ í•™ìŠµí•˜ê³ , ë‹¤ë¥´ë©´ ì „ì´ í•™ìŠµì„ í†µí•´ ì§€ì‹ì„ ì´ì‹í•©ë‹ˆë‹¤.
    """
    X = np.load(X_path)
    y = np.load(y_path)
    new_num_classes = y.shape[1]

    print(f"ğŸ”¹ X shape: {X.shape}")
    print(f"ğŸ”¹ y shape: {y.shape} (ìƒˆë¡œìš´ í´ë˜ìŠ¤ ê°œìˆ˜: {new_num_classes})")

    y_classes = np.argmax(y, axis=1)
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    train_idx, val_idx = next(sss.split(X, y_classes))
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    if CONTINUE_TRAINING and os.path.exists(model_save_path):
        print(f"ğŸ“– ê¸°ì¡´ ëª¨ë¸ '{model_save_path}'ì„(ë¥¼) ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        old_model = load_model(model_save_path)
        old_num_classes = old_model.layers[-1].units
        print(f"   - ê¸°ì¡´ ëª¨ë¸ì˜ í´ë˜ìŠ¤ ê°œìˆ˜: {old_num_classes}")

        if old_num_classes == new_num_classes:
            print("   - í´ë˜ìŠ¤ ê°œìˆ˜ê°€ ë™ì¼í•˜ì—¬, ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
            model = old_model
        else:
            print(f"   - âš ï¸ í´ë˜ìŠ¤ ê°œìˆ˜ê°€ {old_num_classes} -> {new_num_classes} (ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   - ì „ì´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤: ê¸°ì¡´ ëª¨ë¸ì˜ ì§€ì‹ì„ ìƒˆë¡œìš´ êµ¬ì¡°ì˜ ëª¨ë¸ë¡œ ì´ì‹í•©ë‹ˆë‹¤.")
            
            model = Sequential([
                LSTM(64, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2])),
                Dropout(0.2),
                LSTM(128, return_sequences=True, activation='relu'),
                Dropout(0.2),
                LSTM(64, return_sequences=False, activation='relu'),
                Dense(64, activation='relu'),
                Dropout(0.2),
                Dense(new_num_classes, activation='softmax')
            ])
            
            for i, layer in enumerate(model.layers[:-1]):
                if len(layer.get_weights()) > 0 and len(old_model.layers[i].get_weights()) > 0:
                     layer.set_weights(old_model.layers[i].get_weights())

            print("   - ì§€ì‹ ì´ì‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ¨ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")
        model = Sequential([
            LSTM(64, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2])),
            Dropout(0.2),
            LSTM(128, return_sequences=True, activation='relu'),
            Dropout(0.2),
            LSTM(64, return_sequences=False, activation='relu'),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(new_num_classes, activation='softmax')
        ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, verbose=1, mode='min')
    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min')
    log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

    print("\nâ–¶ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    history = model.fit( # ğŸŒŸ í›ˆë ¨ ê¸°ë¡ì„ 'history' ë³€ìˆ˜ì— ì €ì¥
        X_train, y_train,
        epochs=100,
        batch_size=16,
        validation_data=(X_val, y_val),
        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard_callback]
    )

    # --- ğŸ‘‡ ì—¬ê¸°ê°€ ì´ë²ˆì— ì¶”ê°€ëœ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ‘‡ ---
    print("\nâ–¶ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.")
    # í›ˆë ¨ ì¤‘ ì €ì¥ëœ ìµœì ì˜ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    best_model = load_model(model_save_path)
    # ê²€ì¦ ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€
    loss, accuracy = best_model.evaluate(X_val, y_val, verbose=0)
    print(f"   - ìµœì¢… ê²€ì¦ ì†ì‹¤ (Final Validation Loss): {loss:.4f}")
    print(f"   - ìµœì¢… ê²€ì¦ ì •í™•ë„ (Final Validation Accuracy): {accuracy:.4f}")

    # í•™ìŠµ ê³¼ì • ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
    plot_training_history(history, model_save_path)
    # --- ğŸ‘† ì—¬ê¸°ê¹Œì§€ê°€ ì¶”ê°€ëœ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ‘† ---

    print(f"\nâœ… ì–‘ì† LSTM ëª¨ë¸ í•™ìŠµ ë° ëª¨ë“  ê³¼ì • ì™„ë£Œ: {model_save_path}")

if __name__ == "__main__":
    train_lstm_model_dual(
        X_path="processed_lstm/X_seq_lstm_dual.npy",
        y_path="processed_lstm/y_seq_lstm_dual.npy",
        model_save_path="models/gesture_lstm_model_dual_v4.h5"
    )

